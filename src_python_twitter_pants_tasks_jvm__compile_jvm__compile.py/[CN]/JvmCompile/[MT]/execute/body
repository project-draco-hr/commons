def execute(self, targets):
    relevant_targets = [t for t in targets if t.has_sources(self._file_suffix)]
    if (not relevant_targets):
        return
    egroups = self.context.products.get_data('exclusives_groups')
    group_id = egroups.get_group_key_for_target(relevant_targets[0])
    for conf in self._confs:
        egroups.update_compatible_classpaths(group_id, [(conf, self._resources_dir)])
    classpath = egroups.get_classpath_for_group(group_id)
    for conf in self._confs:
        for jar in self.extra_classpath_elements():
            classpath.insert(0, (conf, jar))
    sources_by_target = self._compute_sources_by_target(relevant_targets)
    with self.invalidated(relevant_targets, invalidate_dependents=True, partition_size_hint=self._partition_size_hint) as invalidation_check:
        if (invalidation_check.invalid_vts and (not self.dry_run)):
            invalid_targets = [vt.target for vt in invalidation_check.invalid_vts]
            invalid_sources_by_target = {}
            for tgt in invalid_targets:
                invalid_sources_by_target[tgt] = sources_by_target[tgt]
            invalid_sources = list(itertools.chain.from_iterable(invalid_sources_by_target.values()))
            deleted_sources = self._deleted_sources()
            self._ensure_analysis_tmpdir()
            tmpdir = os.path.join(self._analysis_tmpdir, str(uuid.uuid4()))
            os.mkdir(tmpdir)
            valid_analysis_tmp = os.path.join(tmpdir, 'valid_analysis')
            newly_invalid_analysis_tmp = os.path.join(tmpdir, 'newly_invalid_analysis')
            invalid_analysis_tmp = os.path.join(tmpdir, 'invalid_analysis')
            if self._analysis_parser.is_nonempty_analysis(self._analysis_file):
                with self.context.new_workunit(name='prepare-analysis'):
                    self._analysis_tools.split_to_paths(self._analysis_file, [((invalid_sources + deleted_sources), newly_invalid_analysis_tmp)], valid_analysis_tmp)
                    if self._analysis_parser.is_nonempty_analysis(self._invalid_analysis_file):
                        self._analysis_tools.merge_from_paths([self._invalid_analysis_file, newly_invalid_analysis_tmp], invalid_analysis_tmp)
                    else:
                        invalid_analysis_tmp = newly_invalid_analysis_tmp
                    self.move(valid_analysis_tmp, self._analysis_file)
                    self.move(invalid_analysis_tmp, self._invalid_analysis_file)
            valid_targets = list((set(relevant_targets) - set(invalid_targets)))
            self._register_products(valid_targets, sources_by_target, self._analysis_file)
            partitions = []
            for vts in invalidation_check.invalid_vts_partitioned:
                partition_tmpdir = os.path.join(tmpdir, Target.maybe_readable_identify(vts.targets))
                os.mkdir(partition_tmpdir)
                sources = list(itertools.chain.from_iterable([invalid_sources_by_target.get(t, []) for t in vts.targets]))
                analysis_file = os.path.join(partition_tmpdir, 'analysis')
                partitions.append((vts, sources, analysis_file))
            if (self._analysis_parser.is_nonempty_analysis(self._invalid_analysis_file) and partitions):
                with self.context.new_workunit(name='partition-analysis'):
                    splits = [(x[1], x[2]) for x in partitions]
                    self._analysis_tools.split_to_paths(self._invalid_analysis_file, splits)
            for partition in partitions:
                (vts, sources, analysis_file) = partition
                cp_entries = [entry for (conf, entry) in classpath if (conf in self._confs)]
                self._process_target_partition(partition, cp_entries)
                if os.path.exists(analysis_file):
                    new_valid_analysis = (analysis_file + '.valid.new')
                    if self._analysis_parser.is_nonempty_analysis(self._analysis_file):
                        with self.context.new_workunit(name='update-upstream-analysis'):
                            self._analysis_tools.merge_from_paths([self._analysis_file, analysis_file], new_valid_analysis)
                    else:
                        shutil.copy(analysis_file, new_valid_analysis)
                    self.move(new_valid_analysis, self._analysis_file)
                    self._register_products(vts.targets, sources_by_target, analysis_file)
                    if self._dep_analyzer:
                        actual_deps = self._analysis_parser.parse_deps_from_path(analysis_file, (lambda : self._compute_classpath_elements_by_class(cp_entries)))
                        with self.context.new_workunit(name='find-missing-dependencies'):
                            self._dep_analyzer.check(sources, actual_deps)
                    if self.artifact_cache_writes_enabled():
                        self._write_to_artifact_cache(analysis_file, vts, invalid_sources_by_target)
                if self._analysis_parser.is_nonempty_analysis(self._invalid_analysis_file):
                    with self.context.new_workunit(name='trim-downstream-analysis'):
                        new_invalid_analysis = (analysis_file + '.invalid.new')
                        discarded_invalid_analysis = (analysis_file + '.invalid.discard')
                        self._analysis_tools.split_to_paths(self._invalid_analysis_file, [(sources, discarded_invalid_analysis)], new_invalid_analysis)
                        self.move(new_invalid_analysis, self._invalid_analysis_file)
                vts.update()
        else:
            self._register_products(relevant_targets, sources_by_target, self._analysis_file)
    for conf in self._confs:
        egroups.update_compatible_classpaths(group_id, [(conf, self._classes_dir)])
    self.post_process(relevant_targets)
